{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\google\\protobuf\\descriptor.py:47: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from google.protobuf.pyext import _message\n"
     ]
    }
   ],
   "source": [
    "# Run in terminal or command prompt to download spacy dict\n",
    "# python3 -m spacy download en\n",
    "# copyright Felipe Castrollio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# topics library\n",
    "from topics import prepare_topics\n",
    "from topics import print_top_words_per_topic\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Keras tools\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from keras.engine.input_layer import Input\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "\n",
    "window_size = 10\n",
    "epochs = 200000\n",
    "n_topics = 22 # for lda model\n",
    "vector_dim = 100\n",
    "batch_size = 1000\n",
    "\n",
    "# validation \n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 50  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\defusedxml\\ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\defusedxml\\ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\defusedxml\\ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\defusedxml\\ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\defusedxml\\ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\defusedxml\\ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\defusedxml\\ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Domestic_equity.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>accession#</th>\n",
       "      <th>filing_year</th>\n",
       "      <th>principal_strategies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000894189-10-001729</td>\n",
       "      <td>2010</td>\n",
       "      <td>of the fund under normal market conditions, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0000893730-10-000026</td>\n",
       "      <td>2010</td>\n",
       "      <td>under normal market conditions, the fund will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0000950123-10-067967</td>\n",
       "      <td>2010</td>\n",
       "      <td>of the fund the fund invests, under normal cir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0000950123-10-067966</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0000950123-10-067964</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            accession#  filing_year  \\\n",
       "0           0  0000894189-10-001729         2010   \n",
       "1           1  0000893730-10-000026         2010   \n",
       "2           2  0000950123-10-067967         2010   \n",
       "3           3  0000950123-10-067966         2010   \n",
       "4           4  0000950123-10-067964         2010   \n",
       "\n",
       "                                principal_strategies  \n",
       "0  of the fund under normal market conditions, th...  \n",
       "1  under normal market conditions, the fund will ...  \n",
       "2  of the fund the fund invests, under normal cir...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17484"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ for explanation on data processing steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-11-18e7babe455d>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
      "<ipython-input-11-18e7babe455d>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. prior to the date of this prospectus, the portfolio had not commenced '\n",
      " 'operations and did not have a portfolio turnover rate. principal investment '\n",
      " 'strategies under normal market circumstances, the portfolio seeks to provide '\n",
      " 'exposure to a number of different alternative investment strategies '\n",
      " '(“alternative risk premia”) using both long and short positions and '\n",
      " 'derivatives-based positions across multiple asset classes alternative risk '\n",
      " 'premia portfolio summary prospectus—december 19, 2018 ticker symbol: glarx '\n",
      " 'including, but not limited to, equities, options on u.s. and/or foreign '\n",
      " 'securities, indices and fixed income securities issued or guaranteed by the '\n",
      " 'u.s. treasury (“u.s. treasury securities”) and commodities. the portfolio '\n",
      " 'may obtain exposure to such asset classes directly or indirectly by '\n",
      " 'investing in exchange-traded funds (“etfs”) and exchange-traded notes '\n",
      " '(“etns”). alternative risk premia are designed to harvest well researched '\n",
      " 'market anomalies and provide exposure to the following different investment '\n",
      " 'styles: “value,” “momentum,” “carry,” “size,” “defensive,” “structural” and '\n",
      " '“volatility.” value styles seek to capture the relative value difference '\n",
      " 'between cheap and expensive assets. momentum styles seek to capture the '\n",
      " 'tendency for recent price movements of an asset to continue. carry styles '\n",
      " 'seek to capture the tendency for higher yielding assets to outperform low '\n",
      " 'yielding assets. size styles seek to capture the tendency for smaller, more '\n",
      " 'nimble companies to outperform larger companies. defensive styles seek to '\n",
      " 'capture higher risk-adjusted returns by owning lower risk and higher quality '\n",
      " 'assets. structural styles seek to profit from structural advantages only '\n",
      " 'available to certain investors, such as owning certain types of securities '\n",
      " 'that are not available to some investors as a result of investment mandates. '\n",
      " 'volatility styles seek to harvest the volatility risk premium embedded in '\n",
      " 'written options as compensation for investors seeking a form of financial '\n",
      " 'insurance. each of the underlying strategies are designed to exhibit low '\n",
      " 'correlation to one another, while also exhibiting low correlation to '\n",
      " 'traditional equities and fixed income securities over a full market cycle. '\n",
      " 'the advisor believes that low correlation enhances diversification benefits '\n",
      " 'by reducing long-term overall portfolio risk while retaining the potential '\n",
      " 'for long-term portfolio returns. the portfolio’s exposures to the investment '\n",
      " 'styles and asset classes will vary based on the advisor’s ongoing evaluation '\n",
      " 'of investment opportunities and market conditions. the portfolio expects to '\n",
      " 'maintain exposure to all seven investment styles, in varying allocations; '\n",
      " 'however, not all styles will be represented within each asset class. using '\n",
      " 'quantitative analysis, the portfolio invests in long and short positions '\n",
      " 'with respect to equity securities, such as common stocks, of u.s. public '\n",
      " 'companies, and commodities either directly or with derivatives. the '\n",
      " 'portfolio will invest in companies with market capitalizations, at the time '\n",
      " 'of purchase, that are within the market capitalization range of any stock in '\n",
      " 'the russell 3000 ® index. that capitalization range was $159.2 million to '\n",
      " '$926.9 billion as of may 11, 2018. the portfolio will initially obtain '\n",
      " 'exposure to commodities by investing in etfs and etns that seek to track the '\n",
      " 'performance of commodity futures indices. the portfolio may also invest '\n",
      " 'directly in such futures or indirectly by forming and investing in a '\n",
      " 'wholly-owned cayman islands subsidiary (the “subsidiary”) to invest in those '\n",
      " 'instruments. the portfolio may allocate up to 25% of its assets in the '\n",
      " 'subsidiary, which will have the same investment objective as the portfolio, '\n",
      " 'and will be intended to provide the portfolio with indirect exposure to '\n",
      " 'futures contracts and commodities in a manner consistent with the '\n",
      " 'limitations and requirements of the internal revenue code of 1986, as '\n",
      " 'amended (the “code”) that apply to the portfolio, which limit the amount of '\n",
      " 'income the portfolio may receive from certain sources. to the extent they '\n",
      " 'are applicable to the investment activities of the subsidiary, the '\n",
      " 'subsidiary will be subject to the same investment restrictions and '\n",
      " 'limitations, and follow the same compliance policies and procedures, as the '\n",
      " 'portfolio. the portfolio uses option writing strategies in an effort to '\n",
      " 'obtain option premiums and reduce risk. the portfolio will implement '\n",
      " 'buy-write (covered call) and/or cash-secured put option strategies (“options '\n",
      " 'strategies”) on u.s. or foreign stock index etfs, u.s. or foreign stock '\n",
      " 'indices and/or individual u.s. or foreign stocks held by the portfolio. the '\n",
      " 'portfolio will also implement options strategies on etfs that seek to track '\n",
      " 'the performance of long-term u.s. treasury securities indices. covered call '\n",
      " 'and cash-secured put options are intended to reduce volatility, earn option '\n",
      " 'premiums and provide more stable returns. selling call options reduces the '\n",
      " 'risk of owning stocks by the receipt of the option premiums and selling put '\n",
      " 'options reduces the purchase price of the underlying stock, but both '\n",
      " 'strategies limit the opportunity to profit from an increase in the market '\n",
      " 'value of the underlying security in exchange for up-front cash at the time '\n",
      " 'of selling the call or put option. the call and put options the portfolio '\n",
      " 'writes will be covered by owning the u.s. or foreign security, u.s. treasury '\n",
      " 'securities etfs, or other etfs underlying the option, holding an offsetting '\n",
      " 'option, segregating cash or other liquid assets at not less than the full '\n",
      " 'value of the option or the exercise price, and/or using other permitted '\n",
      " 'coverage methods. to the extent the portfolio’s assets are subject to '\n",
      " 'covered calls on an index, the portfolio may hold etfs instead of individual '\n",
      " 'stocks that replicate the movement of the applicable index, in addition to '\n",
      " 'other permitted coverage methods. a portion of the portfolio’s assets may '\n",
      " 'consist of cash or cash equivalents, including, but not limited to, u.s. '\n",
      " 'treasury bills. the portfolio may invest in companies with small, medium or '\n",
      " 'large market capitalizations in developed, developing or emerging markets in '\n",
      " 'advancement of its investment objective. the portfolio intends to invest in '\n",
      " 'foreign securities in the form of american depositary receipts (“adrs”) '\n",
      " 'which are securities issued by a u.s. bank that represent interests in '\n",
      " 'foreign equity securities listed on a u.s. stock exchange. the portfolio may '\n",
      " 'also buy call and put options on u.s. or foreign stock index etfs, u.s. '\n",
      " 'treasury securities etfs, u.s. or foreign stock indices and/or individual '\n",
      " 'u.s. or foreign stocks. the portfolio generally buys puts in anticipation of '\n",
      " 'a decline in the market value of the underlying instrument, as a substitute '\n",
      " 'for selling a security short. 2 alternative risk premia portfolio summary '\n",
      " 'prospectus—december 19, 2018 ticker symbol: glarx the investment techniques '\n",
      " 'employed by the portfolio create leverage. as a result, the sum of the '\n",
      " 'portfolio’s investment exposures will typically exceed the amount of the '\n",
      " 'portfolio’s net assets. these exposures may vary over time. the advisor '\n",
      " 'expects gross notional exposure of the portfolio to be in a range of 300% to '\n",
      " '400% of the net asset value of the portfolio under normal market conditions; '\n",
      " 'leverage may be significantly different (higher or lower) as deemed '\n",
      " 'necessary by the advisor. the advisor’s selection of securities to buy, sell '\n",
      " 'or borrow is based on a combination of proprietary multifactor computer '\n",
      " 'models and fundamental analysis. the computer models rank securities based '\n",
      " 'on certain criteria, including valuation ratios, profitability and earnings '\n",
      " 'related measures, and other models focus on risk analysis and overall '\n",
      " 'portfolio characteristics. the advisor buys securities or takes long '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'positions in securities that the models identify as undervalued and more '\n",
      " 'likely to appreciate, and sells securities or takes short positions in '\n",
      " 'securities that the advisor identifies as overvalued and more likely to '\n",
      " 'depreciate. the advisor will determine the size of each long or short '\n",
      " 'position and its impact on the risk to the overall portfolios. the frequency '\n",
      " 'and size of short sales will vary substantially in different periods as '\n",
      " 'market opportunities change. the portfolio and subsidiary may actively trade '\n",
      " 'their securities to achieve their principal investment strategies. the '\n",
      " 'advisor will attempt to minimize the impact of federal and state income '\n",
      " 'taxes on shareholders’ returns by, for example, selling depreciated '\n",
      " 'investments to offset capital gains. principal']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.principal_strategies.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "pprint(data[-2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17484\n",
      "fund normal market condition fund invest least net asset equity security primarily invest equity security pay expect pay dividend fund invest company size large well establish company small company advisor select dividend pay equity security consist common stock security have characteristic common stock such preferred stock convertible security right warrant basis fundamental corporate analysis fund also invest foreign security provide publicly trade include american depositary receipt advisor screen universe more stock order identify low price earning ratio price book value price revenue ratio relative historical norm industry peer overall market fund portfolio seek broad diversification exposure significant number major market sector industry group advisor sell position reach approach target price low target price result reassessment earning valuation multiple more attractive stock identify\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "#nlp = spacy.load(\"c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.0.0\", disable=['parser', 'ner'])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(len(data_lemmatized))\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize every doc\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_lemmatized)\n",
    "sequences = tokenizer.texts_to_sequences(data_lemmatized)\n",
    "n_documents = len(sequences)\n",
    "\n",
    "dictionary = tokenizer.word_index\n",
    "dictionary[\"null\"] = 0\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "vocab_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 815, 41, 8, 297, 187, 42, 4, 8, 191, 144, 189, 119, 22, 94, 77, 99, 209, 173, 76, 66, 322, 4, 33, 1616, 191, 119, 1112, 71, 117, 1, 5, 46, 7, 8, 1, 25, 493, 3, 76]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 1, 5, 46, 40, 7, 10, 2, 4, 42, 6, 16]\n",
      "Error on [138, 1, 5, 46, 40, 7, 10, 2, 181, 3, 12, 11, 66, 459, 5, 8, 12, 11, 1, 102, 669, 197, 9, 41, 8, 663, 11, 35, 109, 50, 302, 6, 16, 286]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 84, 89, 69, 17, 84, 89, 181, 3, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 1, 5, 46, 40, 7, 41, 8, 31, 16, 4, 1, 5, 40, 7, 10, 2, 4, 42, 6, 16]\n",
      "Error on [138, 1, 5, 46, 40, 7, 10, 2, 42, 16, 53, 1, 5, 7, 2, 170, 31, 16, 53, 1, 10, 2, 342, 47, 41, 8, 1, 5, 17, 199, 10, 2, 9, 60, 33, 98, 106, 209, 1, 5, 32, 2]\n",
      "1000\n",
      "Error on [1, 5, 47, 10, 2, 4, 1, 146, 5, 297, 42, 16, 4, 30, 5, 31, 16, 4, 1, 5, 40, 7, 2, 32, 53]\n",
      "Error on [138, 1, 5, 46, 40, 7, 41, 8, 31, 16, 4]\n",
      "Error on [1, 5, 47, 10, 2, 4, 1, 5, 40, 7, 2, 32, 53, 194, 622, 188, 158, 155, 118, 2]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 6, 55, 1, 5, 46, 40, 7, 10, 2, 4, 6, 16, 517, 19, 56, 1, 5, 40, 7, 10, 2]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 234, 3, 68, 5, 47, 10, 2, 4, 20, 211, 454, 284, 241, 6, 53, 708, 330, 1095, 659, 88, 71, 117, 1, 5, 46, 40, 7, 10, 2, 42, 44, 4, 1, 29, 174, 926, 101, 46, 250, 753, 1, 5, 148, 7, 10, 2, 32, 4, 65, 88, 12, 245, 200, 188, 158, 10, 2, 32, 8, 65, 32, 88, 42, 44, 4, 4, 19, 56, 6, 16, 59, 4, 411, 324, 20, 11, 31, 4, 20, 11, 480, 6, 16, 42, 4, 291, 480, 6, 16, 70, 1, 3, 24, 40, 7, 12, 130, 3, 70, 267, 1, 91, 68]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 8, 94, 144, 74, 20, 77, 171, 156, 6, 28, 1, 9, 342, 1115, 42, 122, 16, 8]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 10, 2, 4, 42, 6, 16]\n",
      "Error on []\n",
      "Error on [1, 5, 46, 7, 41, 8, 297, 187, 4, 85, 3, 76, 5, 297, 187, 4, 33, 337, 211, 57, 20, 1, 25, 493, 3, 76]\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 31, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 41, 8, 4, 297, 6, 16, 70, 1, 4, 6, 16, 164, 4, 862, 11, 587, 5, 4, 31, 42, 6, 16, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 176, 4, 850, 4, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 4, 4, 1215, 529, 150, 77, 20, 138, 5, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "2000\n",
      "Error on [138, 5, 47, 41, 8, 120, 465, 334, 4, 138, 5, 46, 7, 1177, 1268, 4, 4, 8, 12, 11, 4, 6, 16, 46, 12, 11, 5, 2, 4, 4, 1215, 529, 201, 762, 605, 83, 26, 7, 172, 74, 20, 77, 36, 152, 605, 2, 17, 4, 215, 69, 8, 4, 538, 227, 15, 8, 5, 2, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [9, 5, 47, 10, 2, 236, 41, 8, 4, 3, 60, 33, 201, 64, 74, 36, 152, 7, 15, 9, 66, 125, 42, 44, 4, 6, 16, 53, 9, 5, 224, 6, 55, 9, 30, 5, 122, 44, 31, 44, 4, 71, 117, 9, 5, 46, 7, 10, 2, 4, 9, 5, 73, 7, 2, 111, 4, 12, 245, 188, 158, 200, 188, 158, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 1, 197, 10, 1, 5, 346, 298, 17, 3043, 8, 231, 1, 651, 182, 2, 18, 1, 214, 342, 42, 122, 31, 16, 8, 134, 4]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 165, 785, 52, 729, 3, 112, 127, 265, 81, 1042, 42, 44, 11, 11, 445, 523, 238, 6, 16, 252, 11, 127, 212, 10, 6, 81, 42, 16, 8, 1, 193, 530, 23, 11, 5, 336, 7, 8, 103, 11, 102, 8, 93, 215, 598, 302, 11]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 6, 55, 1, 5, 46, 40, 7, 10, 2, 4, 6, 16, 517, 19, 56, 1, 5, 40, 7, 10, 2]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 10, 2, 297, 16, 4, 1, 5, 40, 7, 2, 32, 4, 1, 315, 138, 5, 9, 8, 85, 45, 35, 20, 77, 1, 5, 41, 8, 5, 17, 199, 10, 2]\n",
      "Error on [91, 3, 68]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 2, 12, 85, 84, 89, 2, 11, 11, 375, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "3000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 27, 300, 5, 47, 8, 85, 52, 207, 77, 99, 209, 71, 117, 1, 5, 46, 7, 41, 8, 122, 16, 4, 1, 5, 73, 7, 2, 32, 53, 111, 278, 2, 1, 389, 72, 136, 4, 599, 123, 72, 136, 4, 12, 881, 58, 69, 195, 277, 473, 72, 37, 10, 195, 52, 179, 1, 223, 122, 16, 4, 4, 6, 16, 1886, 59, 11, 59, 93, 463, 21]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 10, 2, 17, 54, 164, 109, 123, 4, 42, 6, 16]\n",
      "Error on [1, 5, 46, 7, 41, 8, 122, 187, 4, 85, 3, 76, 5, 122, 187, 4, 33, 337, 211, 57, 20, 1, 25, 493, 3, 76]\n",
      "Error on [5, 184, 18, 450, 134, 10, 1, 166, 10, 1, 67, 1, 34, 35, 1, 25, 820, 7, 13, 24, 127, 90, 87, 304, 61, 159, 7, 18, 450, 1, 328, 7, 13, 24, 175, 569, 233, 266, 93, 134, 10, 1, 166, 10, 1, 67, 1, 34, 35, 1, 93, 61, 61, 25, 7, 13, 18, 450, 1, 93, 100]\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 42, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 46, 7, 10, 2, 138, 5, 47, 41, 8, 5, 4, 4, 1215, 529, 144, 20, 77, 8, 4, 538, 227, 20, 8, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on [138, 5, 46, 7, 41, 8, 4, 297, 6, 16, 70, 1, 4, 6, 16, 164, 4, 862, 11, 587, 5, 4, 31, 42, 6, 16, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 31, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 42, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 4, 4, 1215, 529, 144, 20, 77, 8, 4, 538, 227, 20, 8, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 176, 4, 850, 4, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 4, 4, 1215, 529, 150, 77, 20, 138, 5, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 127, 945, 172, 1308, 353, 757, 1204, 1308, 1308, 353, 1221, 1308, 54, 1308, 772, 1204, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "4000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 15, 40, 7, 130, 3, 70, 41, 8, 17, 10, 2, 31, 16, 4, 17, 3, 164, 109, 123, 1, 5, 40, 7, 10, 2, 4, 42, 6, 16]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 6, 55, 1, 5, 46, 40, 7, 154, 130, 3, 70, 10, 2, 4, 6, 16, 517, 19, 56, 1, 5, 40, 7, 10, 2]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 10, 2, 297, 16, 4, 17, 3, 164, 109, 123, 1, 5, 40, 7, 2, 32, 4, 1, 315, 138, 5, 417, 31, 239, 8, 85, 45, 35, 20, 77, 1, 5, 41, 8, 5, 17, 199, 10, 2]\n",
      "Error on [1, 165, 729, 3, 112, 127, 265, 81, 11, 11, 212, 81, 31, 16, 8, 1, 193, 530, 23, 11, 5, 336, 7, 8, 103, 11, 102, 8, 93, 215, 598, 302, 11]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 815, 122, 42, 16, 8, 4, 1, 76, 322, 4, 8, 33, 195, 74, 20, 77, 1090, 15, 19, 56, 1, 25, 493, 3, 76]\n",
      "Error on []\n",
      "Error on [1, 5, 815, 8, 122, 42, 16, 4, 261, 74, 87, 310, 1002, 144, 4, 6, 1, 25, 493, 3, 76]\n",
      "Error on [1, 71, 6, 55, 1, 5, 46, 40, 7, 130, 3, 70, 41, 8, 19, 3, 1, 5, 10, 2, 12, 41, 8, 210, 8, 2, 149, 41, 210, 8, 149, 2, 1, 125, 47, 2, 77, 99, 20, 22, 295, 15, 196, 5, 27, 120, 334, 201, 4, 1513, 83, 505, 2162, 110, 1, 5, 73, 7, 2, 32, 53, 12, 188, 158, 1, 5, 2, 4, 31, 297, 16]\n",
      "Error on []\n",
      "Error on [125, 135, 6, 50]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "5000\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 10, 2, 17, 54, 164, 109, 123, 4, 42, 6, 16]\n",
      "Error on [20, 1, 27, 99, 20, 5, 41, 8, 4, 1, 197, 3, 69, 4, 66, 147, 15, 3, 196, 1, 322, 4, 310, 261, 74, 389, 4, 65, 137, 14, 406, 311, 15, 146, 247, 4, 162, 28, 74, 493, 162, 28, 36, 152, 72, 119, 161, 1, 5, 2, 31, 269, 4, 129, 120, 120, 1540, 4, 187]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 46, 7, 8, 138, 5, 47, 41, 8, 138, 5, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 41, 8, 4, 297, 6, 16, 70, 1, 4, 6, 16, 164, 4, 862, 11, 587, 5, 4, 31, 42, 6, 16, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 31, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 10, 2, 4, 42, 6, 16, 6, 16, 164, 4, 324, 20, 11, 59, 93, 19, 56, 1, 5, 20, 10, 2, 20, 1433, 295, 183, 10, 2, 4, 77, 20, 99, 74, 87, 144]\n",
      "Error on [138, 5, 47, 41, 8, 587, 5, 148, 7, 67, 12, 162, 136, 95, 2, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [483, 1, 259, 483, 1, 167, 100, 3, 24, 9, 29, 101, 357, 9, 215, 3, 68, 385, 483, 1, 3, 9, 7, 176, 48, 126, 483, 1, 63, 174, 9, 101, 1326, 533, 9, 532, 3, 483, 1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 154, 130, 3, 70, 751, 44, 8, 1, 66, 94, 8, 751, 44, 8, 19, 56, 6, 16, 59, 16, 4, 11, 59, 6, 16, 1, 956, 2, 971, 56, 593, 8, 2401, 16, 121, 1, 5, 184, 15, 390, 20, 390, 8]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 47, 10, 2, 4, 1, 146, 5, 297, 42, 16, 4, 30, 5, 31, 16, 4, 1, 5, 40, 7, 10, 2, 32, 53]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "6000\n",
      "Error on [5, 184, 18, 450, 1, 786, 108, 6, 1, 27, 98, 3, 141, 936, 73, 37, 217, 11, 6, 402, 1, 25, 820, 7, 13, 24, 127, 90, 971, 97, 25, 7, 13, 18, 450, 134, 10, 1, 12, 104, 166, 10, 1, 67, 1, 12, 240, 274, 2, 34, 35, 1, 2678, 93, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 2, 12, 85, 84, 89, 2, 11, 11, 375, 2, 479, 22, 1, 100]\n",
      "Error on [103, 449, 3, 7, 36, 36, 248, 1, 137, 394, 91, 3, 68, 62, 452, 100, 21]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [91, 3, 68]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 8, 94, 144, 74, 20, 77, 171, 156, 6, 28, 1, 9, 342, 1115, 42, 122, 16, 8]\n",
      "Error on [1, 165, 729, 3, 112, 127, 265, 81, 354, 415, 11, 930, 659, 222, 8, 6, 81, 1831, 8, 42, 4, 1, 193, 530, 23, 11, 5, 336, 7, 8, 103, 11, 102, 8, 93, 215, 598, 302, 11]\n",
      "Error on [1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 815, 42, 122, 16, 4, 8, 94, 76, 201, 201, 8, 66, 389, 90, 76, 1582, 65, 28, 144, 605, 212, 26, 74, 467, 15, 1, 25, 493, 3, 76]\n",
      "Error on [98, 101, 46, 250, 293, 100]\n",
      "Error on [138, 5, 46, 7, 10, 2, 138, 5, 47, 41, 8, 138, 5, 47, 4, 191, 119, 4, 1215, 529, 77, 191, 119, 57, 457, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 237, 25, 283, 177, 114, 176, 98, 368, 398, 847, 753, 931, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "7000\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 471, 99, 142, 94, 276, 1357, 101, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 127, 788, 464, 337, 176, 1881, 69, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [100]\n",
      "Error on [1, 5, 47, 10, 2, 42, 4, 85, 20, 77, 1, 5, 47, 2, 5, 73, 7, 32, 2, 12, 2, 170, 105, 6]\n",
      "Error on [1, 5, 7, 47, 346, 9, 10, 2, 85, 20, 77, 1, 5, 47, 2, 5, 73, 7, 32, 2, 12, 2, 170, 105, 6]\n",
      "Error on [1, 100, 3, 24, 71, 6, 55, 5, 46, 1, 7, 41, 8, 4, 933, 11, 24, 29, 250, 360, 293, 101]\n",
      "Error on [1, 5, 7, 47, 10, 2, 4, 94, 201, 1, 5, 47, 2, 5, 73, 7, 32, 2, 12, 2, 170, 105, 6]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 6, 55, 1, 5, 46, 976, 40, 7, 154, 130, 3, 70, 10, 2, 4, 6, 16, 517, 19, 56, 1, 5, 40, 7, 10, 2]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "8000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 2, 12, 85, 84, 89, 2, 11, 11, 375, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1077, 217, 2, 3233, 372, 1, 854, 213, 792, 100, 111]\n",
      "Error on [12, 466, 452, 45, 1, 121, 130, 39, 73, 7, 12, 154, 466, 43, 2, 34, 130, 94, 622, 352]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 165, 729, 3, 112, 127, 265, 81, 2014, 42, 44, 11, 669, 197, 11, 42, 4, 155, 93, 520, 6, 16, 1, 193, 530, 23, 11, 5, 336, 7, 8, 103, 11, 102, 8, 93, 215, 598, 302, 11]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 10, 2, 17, 3, 164, 109, 123, 12, 11, 1, 66, 459, 5, 8, 12, 11, 1, 102, 669, 197, 9, 41, 8, 663, 11, 35, 109, 50, 302, 6, 16, 286]\n",
      "Error on []\n",
      "Error on [1, 5, 46, 7, 41, 8, 122, 187, 4, 85, 3, 76, 5, 122, 187, 4, 33, 337, 211, 57, 20, 1, 25, 493, 3, 76]\n",
      "Error on [98, 101, 46, 250, 293, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 42, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 31, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 84, 89, 69, 17, 84, 89, 181, 3, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "9000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 31, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on [1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 471, 99, 142, 94, 276, 1357, 101, 100]\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 162, 28, 8, 28, 126, 505, 3, 31, 297, 404, 4, 587, 5, 8, 94, 162, 28, 457, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 98, 244, 340, 176, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [483, 1, 259, 483, 1, 167, 100, 3, 24, 9, 29, 101, 357, 9, 215, 3, 68, 385, 483, 1, 3, 9, 7, 176, 48, 126, 483, 1, 63, 174, 9, 101, 1326, 533, 9, 532, 3, 483, 1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 12, 154, 130, 3, 70, 41, 8, 173, 71, 117, 1, 5, 46, 40, 7, 10, 2, 4, 42, 6, 16, 66, 1, 3, 60, 33, 201, 77, 45, 35, 20, 156, 22, 1, 30, 5, 40, 7, 32, 2, 1, 5, 194, 32, 2, 565, 188, 158]\n",
      "Error on [1, 5, 7, 47, 10, 2, 4, 94, 201, 1, 5, 47, 2, 5, 73, 7, 32, 2, 12, 2, 170, 105, 6]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 47, 10, 2, 4, 1, 146, 5, 297, 42, 16, 4, 30, 5, 31, 16, 4, 1, 5, 40, 7, 10, 2, 32, 53, 790, 209, 9]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "10000\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 2, 12, 85, 84, 89, 2, 11, 11, 375, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [12, 466, 452, 45, 1, 121, 130, 39, 73, 7, 12, 154, 466, 43, 2, 34, 130, 94, 622, 352]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [87, 1, 66, 180, 40, 45, 38, 6, 15, 1, 45, 58, 6, 15, 1, 34, 58, 235, 71, 6, 55]\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 8, 94, 144, 74, 20, 77, 171, 156, 6, 28, 1, 9, 342, 1115, 42, 122, 16, 8]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 165, 729, 3, 112, 127, 265, 81, 354, 415, 11, 930, 659, 222, 8, 6, 81, 1831, 8, 42, 4, 1, 193, 530, 23, 11, 5, 336, 7, 8, 103, 11, 102, 8, 93, 215, 598, 302, 11]\n",
      "Error on [1]\n",
      "Error on []\n",
      "Error on []\n",
      "11000\n",
      "Error on [71, 117, 1, 27, 91, 3, 68, 5, 46, 7, 122, 16, 4, 125, 2, 79, 529, 201, 762, 1, 223, 122, 16, 4, 4, 6, 16, 1886, 59, 11, 59, 93, 1, 5, 40, 7, 2, 32, 53, 111, 278, 2]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 10, 2, 17, 54, 164, 109, 123, 4, 42, 6, 16]\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 10, 2, 17, 3, 164, 109, 123, 12, 11, 1, 66, 459, 5, 8, 12, 11, 1, 102, 669, 197, 9, 41, 8, 663, 11, 35, 109, 50, 302, 6, 16, 286]\n",
      "Error on [1, 5, 815, 42, 16, 8, 4, 94, 144, 74, 20, 77, 489, 8, 28, 1342, 87, 74, 71, 117, 46, 1, 7, 5, 2, 170, 4, 1, 25, 493, 3, 76]\n",
      "Error on [98, 101, 46, 250, 293, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 41, 8, 4, 2456, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on [5, 47, 41, 8, 42, 863, 144, 6, 16, 66, 62, 76, 33, 126, 28, 668, 96, 74, 80, 427, 4, 1, 66, 102, 182, 3, 61, 1893, 2, 43, 416, 56, 117, 135, 4, 69, 321, 6, 29]\n",
      "Error on [3, 24, 1, 5, 47, 10, 2, 42, 16, 4, 85, 20, 77, 1, 5, 47, 2, 5, 73, 7, 32, 2, 12, 2, 170, 105, 6]\n",
      "Error on [3, 24, 1, 138, 5, 46, 7, 41, 8, 46, 1, 7, 138, 5, 41, 8, 4, 191, 119, 174, 29, 46, 250, 360, 293, 101, 1, 5, 47, 2, 5, 73, 7, 32, 2, 12, 2, 105, 6]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 41, 8, 12, 324, 20, 11, 6, 16, 252, 11, 127, 212, 81, 42, 44, 20, 402, 10, 6, 66, 25, 873, 1244, 168, 115, 567, 110, 20, 535, 17, 83, 85, 669, 197, 298, 8, 77, 98, 72, 73, 37, 20, 11, 457, 134, 32, 53, 375, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "12000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 2, 12, 85, 84, 89, 2, 11, 11, 375, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "13000\n",
      "Error on [87, 1, 66, 180, 40, 45, 38, 6, 15, 1, 45, 58, 6, 15, 1, 34, 58, 235, 71, 6, 55]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 85, 84, 89, 2, 11, 11, 375, 2, 479, 22, 1, 100]\n",
      "Error on [138, 5, 46, 7, 41, 8, 42, 16, 4, 12, 11, 375, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [98, 101, 46, 250, 293, 100]\n",
      "Error on [138, 5, 47, 41, 8, 5, 4, 4, 1215, 529, 144, 20, 77, 8, 4, 538, 227, 20, 8, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 369, 945, 172, 1137, 176, 1137, 757, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "14000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 1, 197, 10, 1, 5, 346, 298, 17, 3043, 10, 231, 1, 651, 182, 2, 18, 1, 214, 815, 342, 42, 122, 31, 16, 10, 2, 134, 4]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 6, 55, 1, 5, 46, 976, 40, 7, 154, 130, 3, 70, 10, 2, 4, 6, 16, 517, 19, 56, 1, 5, 40, 7, 10, 2]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 84, 89, 69, 17, 84, 89, 181, 3, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "15000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 678, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 2, 12, 324, 20, 11, 6, 16, 252, 11, 127, 212, 81, 42, 44, 20, 402, 10, 6, 678, 2, 479, 22, 1, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 43, 2, 17, 706, 614, 142, 121, 213, 672, 7, 62, 14, 107, 602, 106, 86, 36, 189, 1, 9]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 41, 8, 4, 42, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 5, 8, 94, 144, 74, 20, 77, 171, 156, 6, 28, 1, 9, 342, 1115, 42, 122, 16, 8]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 47, 346, 9, 41, 8, 524, 216, 59, 69, 4, 6, 16, 79, 33, 516, 20, 77, 125, 122, 42, 16, 8, 1, 5, 40, 7, 32, 53, 111, 278, 2, 1, 65, 2, 373, 52, 25, 63, 115, 107, 4, 839, 496, 123, 56, 64, 6, 109, 55, 1, 71, 253, 8, 190, 114, 125, 39, 50, 6]\n",
      "Error on []\n",
      "Error on [1, 5, 815, 42, 16, 8, 4, 94, 144, 74, 20, 77, 489, 8, 28, 1342, 87, 74, 71, 117, 46, 1, 7, 5, 2, 170, 4, 1, 25, 493, 3, 76, 76, 1505, 85, 180, 9, 41, 8, 1]\n",
      "Error on []\n",
      "Error on [1, 27, 45, 35, 99, 20, 5, 41, 8, 4, 1, 197, 3, 69, 4, 1, 322, 4, 310, 261, 74, 389, 4, 65, 137, 14, 406, 311, 15, 146, 247, 4, 162, 28, 74, 493, 162, 28, 36, 152, 72, 119, 161, 1, 236, 5, 2, 4, 6, 16, 150]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 42, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 31, 6, 16, 70, 1, 4, 6, 16, 164, 4, 31, 44, 11, 457, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 46, 7, 10, 2, 138, 5, 47, 41, 8, 5, 2, 4, 4, 1215, 529, 201, 762, 605, 83, 26, 7, 172, 74, 20, 77, 36, 152, 605, 2, 17, 4, 215, 69, 8, 4, 538, 227, 15, 8, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 850, 69, 4, 498, 713, 261, 850, 618, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 46, 7, 8, 138, 5, 47, 41, 8, 138, 5, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 98, 80, 176, 562, 69, 457, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 1373, 457, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 488, 181, 2286, 1137, 176, 177, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "16000\n",
      "Error on []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [3, 24, 1, 5, 7, 47, 346, 9, 10, 2, 85, 20, 77, 1, 5, 47, 2, 5, 73, 7, 32, 2, 12, 2, 170, 105, 6]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [71, 117, 1, 5, 46, 40, 7, 130, 3, 70, 41, 8, 17, 10, 2, 31, 16, 4, 17, 3, 164, 109, 123, 1, 5, 40, 7, 126, 4, 42, 6, 16]\n",
      "Error on [1, 5, 47, 10, 2, 4, 1, 146, 5, 297, 42, 16, 4, 30, 5, 31, 16, 4, 1, 5, 40, 7, 10, 2, 32, 53]\n",
      "Error on [1, 5, 47, 10, 2, 4, 1, 146, 5, 297, 42, 16, 4, 30, 5, 31, 16, 4, 1, 5, 40, 7, 10, 2, 32, 53]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 47, 41, 8, 138, 5, 46, 7, 2, 4, 236, 208, 84, 89, 69, 17, 84, 89, 181, 3, 5, 134, 32, 53, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on []\n",
      "17000\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [255]\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 43, 2, 17, 706, 614, 142, 121, 213, 672, 7, 62, 14, 107, 602, 106, 86, 36, 189, 1, 9]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [1, 3, 1006, 34, 172, 443, 352, 1, 2720, 29, 1, 40, 7, 15, 255]\n",
      "Error on [1, 3, 1006, 34, 172, 443, 352, 1, 2720, 29, 1, 40, 7, 15, 255]\n",
      "Error on [138, 5, 46, 7, 41, 8, 4, 42, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "Error on [138, 5, 46, 7, 41, 8, 4, 42, 6, 16, 70, 1, 4, 6, 16, 164, 4, 11, 5, 134, 32, 53, 5, 20, 8, 15, 8, 25, 63, 115, 83, 26, 53, 80, 55, 69, 58, 129, 120, 6, 109, 55, 85, 3, 100]\n",
      "Error on [255]\n",
      "Error on []\n",
      "Error on []\n",
      "Error on []\n",
      "[[603, 2386], [119, 191], [191, 1548], [2722, 141], [536, 39], [187, 3187], [6, 2454], [158, 2286], [2722, 28], [107, 162]] [0 1 0 1 1 0 0 0 1 1] [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# create dataset: word pairs and doc ids with positive and negative samples\n",
    "\n",
    "window_size = 2\n",
    "targets = []\n",
    "contexts = []\n",
    "labels = []\n",
    "couples = []\n",
    "doc_ids = []\n",
    "\n",
    "for i in range(0,n_documents):\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        print (i)\n",
    "    seq = sequences[i]\n",
    "    sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "    couple, label = skipgrams(seq, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "    if not couple:\n",
    "        next\n",
    "    try:\n",
    "        target, context = zip(*couple)\n",
    "        targets = targets + list(target)\n",
    "        contexts = contexts + list(context)\n",
    "        doc_ids = doc_ids + [i]*len(context)\n",
    "        labels = labels + label\n",
    "        couples = couples + couple\n",
    "    except:\n",
    "        print (\"Error on \" + str(seq))\n",
    "    \n",
    "data_target = np.array(targets, dtype='int32')\n",
    "data_context = np.array(contexts, dtype='int32')\n",
    "doc_ids = np.array(doc_ids, dtype='int32')\n",
    "labels = np.array(labels, dtype='int32')\n",
    "\n",
    "# split into train and test\n",
    "\n",
    "from random import sample\n",
    "training_split = 0.8\n",
    "l = len(data_target) #length of data \n",
    "f = int(l * training_split) #number of elements you need\n",
    "indices = sample(range(l),f)\n",
    "\n",
    "train_data_target = data_target[indices]\n",
    "test_data_target = np.delete(data_target,indices)\n",
    "train_data_context = data_context[indices]\n",
    "test_data_context = np.delete(data_context,indices)\n",
    "train_doc_ids = doc_ids[indices]\n",
    "test_doc_ids = np.delete(doc_ids,indices)\n",
    "train_labels = labels[indices]\n",
    "test_labels = np.delete(labels,indices)\n",
    "\n",
    "print(couples[:10], labels[:10], doc_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data 2159360\n",
      "size of testing data 539840\n",
      "size of labels 2699200\n"
     ]
    }
   ],
   "source": [
    "print(\"size of training data \" + str(len(train_data_target)))\n",
    "print(\"size of testing data \" + str(len(test_data_target)))\n",
    "print(\"size of labels \" + str(len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is where we start creating the model. The model consists of two parallel flows: word embedding (like word2vec) and topic embedding (like LDA). Please refer to the model image here: https://github.com/cemoody/lda2vec. You can see on the left the word embedding happens, and on the right the topic lda embedding happens. At the bottom the two vectors are added together to form the final context_vector. \n",
    "\n",
    "The model will have three training inputs: \n",
    "    1) input_context: pivot word\n",
    "    2) input_target: word that we are trying to predict\n",
    "    3) input_doc: document id \n",
    "\n",
    "And one training output:\n",
    "    1) label: 0 or 1 which defines if input_context and input_target are similar taking into account input_doc\n",
    "    \n",
    "The model predictions are gien by \"preds\" which will output a similarity score between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input placeholder variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "input_doc = Input((1,), dtype='int32')\n",
    "labels = Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_2/Reshape:0' shape=(None, 100, 1) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create word2`vec layers\n",
    "embedding = layers.Embedding(vocab_size, vector_dim, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "word_context = embedding(input_context)\n",
    "word_context = Reshape((vector_dim, 1))(word_context)\n",
    "word_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_3:0\", shape=(None, 1), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding_lookup_v2() got an unexpected keyword argument 'partition_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-98dcf4c8cb95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#doc_topics = all_doc_topics_embedding(input_doc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdoc_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mdoc_topics_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#all_doc_topics_norm = keras.layers.Activation(activation=\"softmax\",name=\"all_doc_topics_norm\")(all_doc_topics)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-98dcf4c8cb95>\u001b[0m in \u001b[0;36membedding_lookup\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_doc_topics_embedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpartition_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mod'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"doc_proportions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#doc_topics = all_doc_topics_embedding(input_doc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: embedding_lookup_v2() got an unexpected keyword argument 'partition_strategy'"
     ]
    }
   ],
   "source": [
    "# create lda layers\n",
    "\n",
    "scalar = 1 / np.sqrt(n_documents + n_topics)\n",
    "all_doc_topics_embedding =(tf.Variable(tf.random.normal([n_documents, n_topics], mean=0, stddev=50*scalar),name=\"doc_embeddings\",trainable=True))  # Gaussian distribution\n",
    "#all_doc_topics_embedding = keras.layers.Embedding(n_documents, n_topics, embeddings_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=1 / np.sqrt(n_documents + n_topics), seed=None))\n",
    "#doc_topics = all_doc_topics_embedding(input_doc)\n",
    "def embedding_lookup(x):\n",
    "    ind = tf.cast(x, tf.int32)\n",
    "    return tf.nn.embedding_lookup(all_doc_topics_embedding,ind,partition_strategy='mod',name=\"doc_proportions\")\n",
    "\n",
    "#doc_topics = all_doc_topics_embedding(input_doc)\n",
    "print(input_doc)\n",
    "doc_topics = keras.layers.Lambda(embedding_lookup)(input_doc)\n",
    "doc_topics_norm = keras.layers.Activation(activation=\"softmax\")(doc_topics)\n",
    "#all_doc_topics_norm = keras.layers.Activation(activation=\"softmax\",name=\"all_doc_topics_norm\")(all_doc_topics)\n",
    "#doc_topics_norm = keras.layers.Lambda(embedding_lookup)(input_doc)\n",
    "transform = keras.layers.Dense(vector_dim, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "topic_context = transform(doc_topics_norm)\n",
    "topic_context = Reshape((vector_dim, 1))(topic_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ee5548bb6814>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# combine context layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_context\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# now perform the dot product operation to get a similarity measure between target and context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topic_context' is not defined"
     ]
    }
   ],
   "source": [
    "# combine context layers\n",
    "context = keras.layers.Add()([word_context, topic_context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure between target and context\n",
    "similarity = layers.dot([target, context], axes=1, normalize=True)\n",
    "similarity = Reshape((1,))(similarity)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "preds = Dense(1, activation='sigmoid', name='similarity')(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defnie custom loss functions\n",
    "\n",
    "# lda loss model\n",
    "lmbda = 1.0\n",
    "fraction = 1/100000\n",
    "alpha = None # defaults to 1/n_topics\n",
    "\n",
    "\n",
    "def dirichlet_likelihood(weights, alpha=None):\n",
    "    \n",
    "    num_topics = n_topics\n",
    "    \n",
    "    if alpha is None:\n",
    "        alpha = 1 / num_topics\n",
    "\n",
    "    log_proportions = tf.nn.log_softmax(weights)\n",
    "\n",
    "    loss = (alpha - 1) * log_proportions\n",
    "\n",
    "    #return -tf.reduce_sum(loss) # log-sum-exp\n",
    "    return tf.reduce_sum(loss) # log-sum-exp\n",
    "\n",
    "def loss_lda(y_pred, y_true, topics_layer):\n",
    "    return lmbda*fraction*dirichlet_likelihood(topics_layer)\n",
    "\n",
    "def loss_word2vec(y_pred, y_true):\n",
    "    #return tf.math.add(tf.math.multiply(y_true, (-tf.math.log(y_pred))), \n",
    "    #                   tf.math.multiply((1 - y_true),(-tf.math.log(1 - y_pred))))\n",
    "    return keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "# lda2vec loss\n",
    "def loss_sum(y_pred, y_true, topics_layer):\n",
    "    word2vec_loss = loss_word2vec(y_pred, y_true)\n",
    "    lda_loss = loss_lda(y_pred, y_true, topics_layer)\n",
    "    sum_loss = word2vec_loss + lda_loss\n",
    "    return sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation models which are used to print out similar words during training.\n",
    "# This is not needed for model training, but is used to check model outputs periodically to see if model is working\n",
    "\n",
    "topic_context = Input(shape=(vector_dim, ))\n",
    "topic_similarity = layers.dot([topic_context, word_context], axes=0)\n",
    "topic2words_model = Model(input=[topic_context,input_context], output=topic_similarity)\n",
    "\n",
    "words_similarity = layers.dot([target, word_context], axes=1, normalize=True)\n",
    "nearby_words_model = Model(input=[input_target, input_context], output=words_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_words =['144a','active','adviser','allocation','country','region','arbitrage', \\\n",
    "      'asset','banking','index','passive','beta','bond', 'debt','brokerage', \\\n",
    "      'call','capitalization','cash','commodity','equity','close','collateral',\\\n",
    "      'company', 'interest','sector','conflict','investment', 'obligations', \\\n",
    "      'diversification','market','counterparty','currency','cybersecurity', \\\n",
    "      'derivative', 'diversification', 'expense','volatility','future','government', \\\n",
    "      'hedge', 'liquidity','insurance','issuer','legal','leverage','target', 'date',  \\\n",
    "      'fund','management','mortgage','over','counter','turnover','estate','settlement', \\\n",
    "      'short','swap','tax','yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions to print similar words given a topic and similar words given another word\n",
    "# This is not used for training, but for periodic evaluation of the model\n",
    "\n",
    "class TopicSimilarityCallback:\n",
    "    def run_sim(self, topics):\n",
    "        for i in range(n_topics):\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(topics[i])\n",
    "            nearest = (-sim).argsort()[0:top_k + 1]\n",
    "            log_str = 'Closest words to topic %d:' % i\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_sim(topic):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.reshape(topic,(1,-1))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = topic2words_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "t_sim_cb = TopicSimilarityCallback()\n",
    "\n",
    "\n",
    "class WordsSimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 10  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[0:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = nearby_words_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "w_sim_cb = WordsSimilarityCallback()\n",
    "\n",
    "class SpecificWordsSimilarityCallback:\n",
    "    def run_sim(self, word):\n",
    "        if word not in dictionary:\n",
    "            print('Nearest to %s: Word does not exist in dictionary' % word)\n",
    "            return\n",
    "        word_index = dictionary[word]\n",
    "        top_k = 10  # number of nearest neighbors\n",
    "        sim = self._get_sim(word_index)\n",
    "        nearest = (-sim).argsort()[0:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % word\n",
    "        for k in range(top_k):\n",
    "            close_word = reverse_dictionary[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = nearby_words_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sw_sim_cb = SpecificWordsSimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS PART IS NOT BEING USED\n",
    "# train model using keras (not being used since lda is not training)\n",
    "\n",
    "def train_with_keras(): \n",
    "    \n",
    "    model = Model(inputs=[input_target, input_context, input_doc], outputs=preds)\n",
    "    model.compile(loss=loss_lda2vec(all_doc_topics_embedding), metrics=[loss_word2vec, loss_lda], optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    \n",
    "    iterations = 100000\n",
    "    batch_size = 100\n",
    "    train_loss_every = 100\n",
    "    test_loss_every = 10000\n",
    "    sum_loss = 0\n",
    "    word2vec_loss = 0\n",
    "    lda_loss = 0\n",
    "\n",
    "    for cnt in range(iterations):\n",
    "\n",
    "        # print out training loss\n",
    "        if cnt % train_loss_every == 0 and cnt > 0:\n",
    "            print(\"Iteration {}, average sum_loss={}, average word2vec_loss={}, average lda_loss={}\".format(cnt, sum_loss/train_loss_every, word2vec_loss/train_loss_every, lda_loss/train_loss_every))\n",
    "            print(all_doc_topics_embedding.get_weights()[0][1])\n",
    "            print(K.eval(dirichlet_likelihood(all_doc_topics_embedding))*fraction)\n",
    "            print(softmax(all_doc_topics_embedding.get_weights()[0][0]))\n",
    "\n",
    "        # print out test loss and similar words\n",
    "        if cnt % test_loss_every == 0 and cnt > 0:\n",
    "            t_sim_cb.run_sim(transform.get_weights()[0])\n",
    "            w_sim_cb.run_sim()\n",
    "            test_loss = model.evaluate(x=[test_data_target, test_data_context, test_doc_ids], y=test_labels)\n",
    "            print(\"Iteration {}, test_loss={}\\n\".format(cnt, test_loss))\n",
    "\n",
    "        # training happens here\n",
    "        idx = np.random.randint(0, len(train_labels)-1, batch_size).tolist()\n",
    "        loss =  model.fit(x=[train_data_target[idx], train_data_context[idx], train_doc_ids[idx]], y=train_labels[idx],epochs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I couldn't get the model to train in Keras (error in LDA loss), so I trained it with TensorFlow instead. \n",
    "# Remember that Keras is running TensorFlow in the back end. This is a bit messy since the model was created \n",
    "#in Keras language, but it works well.\n",
    "\n",
    "import math\n",
    "batch_size = 150\n",
    "train_loss_every = 500\n",
    "test_loss_every = 10000\n",
    "\n",
    "# define loss functions to compute\n",
    "loss = loss_sum(preds, labels, all_doc_topics_embedding)\n",
    "loss_topics = loss_lda(preds, labels, all_doc_topics_embedding)\n",
    "loss_words = loss_word2vec(preds, labels)\n",
    "\n",
    "# define gradient descent and initialize variables\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = K.get_session() # get session from keras\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(200000):\n",
    "        \n",
    "        idx = np.random.randint(0, len(train_labels)-1, batch_size).tolist()\n",
    "        # training happens here\n",
    "        losses = sess.run([train_step, loss,loss_topics, loss_words,preds,labels], feed_dict= {input_target:np.reshape(train_data_target[idx],(-1,1)),\n",
    "                                   input_context:np.reshape(train_data_context[idx],(-1,1)),\n",
    "                                   input_doc:np.reshape(train_doc_ids[idx],(-1,1)),\n",
    "                                   labels: np.reshape(train_labels[idx],(-1,1))})\n",
    "            \n",
    "        # print training loss\n",
    "        if i % train_loss_every == 0:\n",
    "            print(\"Iteration {}, average sum_loss={}, average lda_loss={}, average w2v_loss={}\".format(i,np.mean(losses[1]),np.mean(losses[2]),np.mean(losses[3])))\n",
    "        \n",
    "        # print test\\loss and similar words\n",
    "        if i % test_loss_every == 0:\n",
    "            test_loss = sess.run([loss], feed_dict= {input_target:np.reshape(test_data_target,(-1,1)),\n",
    "                                   input_context:np.reshape(test_data_context,(-1,1)),\n",
    "                                   input_doc:np.reshape(test_doc_ids,(-1,1)),\n",
    "                                   labels: np.reshape(test_labels,(-1,1))})\n",
    "            print(\"\\n\\n******Iteration {}, test_loss={}****\\n\\n\".format(i, np.mean(test_loss[0])))\n",
    "            #w_sim_cb.run_sim()\n",
    "            t_sim_cb.run_sim(transform.get_weights()[0])\n",
    "            for word in find_similar_words:\n",
    "                sw_sim_cb.run_sim(word)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary['market']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
